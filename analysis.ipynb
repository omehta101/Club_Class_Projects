{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "def read_data():\n",
    "    '''\n",
    "    loop through all CSVs in data folder and read them into a pandas dataframe\n",
    "    '''\n",
    "    data = {}\n",
    "    for file in os.listdir(\"data\"):\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(\"data/\" + file)\n",
    "            curncy_name = file.split(\" \")[0].split(\"_\")[-1]\n",
    "            df.set_index(df[\"Date\"], inplace=True)\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            df.drop([\"Date\"], axis=1, inplace=True)\n",
    "            data[curncy_name] = df\n",
    "    return data\n",
    "    \n",
    "    \n",
    "data = read_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigen_value_graph(data):\n",
    "    eigvals = np.linalg.eigvals(data.cov())\n",
    "    plt.plot(range(len(eigvals)), eigvals)\n",
    "    plt.xlabel(\"Eigenvalue Index\")\n",
    "    plt.ylabel(\"Eigenvalue Magnitude\")\n",
    "    plt.title(\"Eigenvalues of Predictors\")\n",
    "    plt.show()\n",
    "    plt.savefig(\"Eigenvalues\")\n",
    "\n",
    "\n",
    "def get_reduced_data(x):\n",
    "    pca = PCA(n_components=7)\n",
    "    data_transformed = pca.fit_transform(x)\n",
    "    return pca, data_transformed\n",
    "\n",
    "eigen_value_graph(data)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "def train_regression_models(x_train, x_test, y_train, y_test):\n",
    "    # Initialize the regression models\n",
    "    linear_model = LinearRegression()\n",
    "    decision_tree_model = DecisionTreeRegressor()\n",
    "    random_forest_model = RandomForestRegressor()\n",
    "    svm_model = SVR()\n",
    "\n",
    "    # Train the models using the training data\n",
    "    linear_model.fit(x_train, y_train)\n",
    "    decision_tree_model.fit(x_train, y_train)\n",
    "    random_forest_model.fit(x_train, y_train)\n",
    "    svm_model.fit(x_train, y_train)\n",
    "\n",
    "    # Evaluate the models using the test data\n",
    "    linear_model_pred = linear_model.predict(x_test)\n",
    "    decision_tree_model_pred = decision_tree_model.predict(x_test)\n",
    "    random_forest_model_pred = random_forest_model.predict(x_test)\n",
    "    svm_model_pred = svm_model.predict(x_test)\n",
    "\n",
    "    # Compute the mean squared error of each model\n",
    "    linear_model_mse = mean_squared_error(y_test, linear_model_pred)\n",
    "    decision_tree_model_mse = mean_squared_error(y_test, decision_tree_model_pred)\n",
    "    random_forest_model_mse = mean_squared_error(y_test, random_forest_model_pred)\n",
    "    svm_model_mse = mean_squared_error(y_test, svm_model_pred)\n",
    "\n",
    "\n",
    "    lstm_units = 50\n",
    "    batch_size = 20\n",
    "    epochs = 15\n",
    "    learning_rate = 0.001\n",
    "    x_train_lstm = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test_lstm = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=lstm_units, input_shape=(x_train.shape[1], 1), return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train_lstm, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(x_test_lstm)\n",
    "\n",
    "    # Calculate mean squared error\n",
    "    lstm_model_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Create a dataframe to store the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': ['Linear Regression', 'Decision Tree', 'Random Forest', 'SVM'],\n",
    "        'Mean Squared Error': [linear_model_mse, decision_tree_model_mse, random_forest_model_mse, svm_model_mse],\n",
    "        'Model Object': [linear_model, decision_tree_model, random_forest_model, svm_model]\n",
    "    })\n",
    "    results_df.sort_values(by='Mean Squared Error', inplace=True)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_by_time(data, label, train_size, pca=False):\n",
    "    # Split the data into features (x) and target (y) based on the specified label\n",
    "    x = data.drop(label, axis=1)[:-1]\n",
    "    y = data[label]\n",
    "    y = y.shift(-1)[:-1]\n",
    "    if pca:\n",
    "        _, x = get_reduced_data(x)\n",
    "    \n",
    "    train_index = int(len(data) * train_size)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=1-train_size, shuffle=False)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curncy in list(data.keys()):\n",
    "    x_train, x_test, y_train, y_test = split_data_by_time(data, f\"{curncy} Curncy\", 0.7, pca=True)\n",
    "    print(train_regression_models(x_train, x_test, y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "strat_errors = {}\n",
    "for curncy in list(data.keys()):\n",
    "    x_train, x_test, y_train, y_test = split_data_by_time(data[curncy], f\"{curncy} Curncy\", 0.7, True)\n",
    "    models = train_regression_models(x_train, x_test, y_train, y_test)\n",
    "    model = models.iloc[1][\"Model Object\"]\n",
    "    predicted_returns = model.predict(x_test)\n",
    "    lagged_returns = pd.Series(predicted_returns)\n",
    "    training_predictions = model.predict(x_train)\n",
    "    training_score = accuracy_score([1 if y > 0 else 0 for y in y_train], [1 if y > 0 else 0 for y in training_predictions])\n",
    "    strat_errors[curncy] = training_score\n",
    "    results[curncy] = {'lagged_returns': lagged_returns, 'y_test': y_test, \"model\": models.iloc[0], \"training_predictions\": pd.Series(training_predictions), \"training_score\": training_score, \"y_train\": y_train}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale strat_error values so everything adds up to 1\n",
    "def get_scaled_weights(currencies):\n",
    "    scaled_accuracies = {k: strat_errors[k] if strat_errors[k] > 0.8 else 0 for k in currencies}\n",
    "    scaled_accuracies = {k: v / sum(scaled_accuracies.values()) for k, v in scaled_accuracies.items()}\n",
    "    return scaled_accuracies\n",
    "\n",
    "def get_scaled_weights_wiht_shorts(currencies):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaled_accuracies = strat_errors\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    accuracies_array = np.array(list(scaled_accuracies.values())).reshape(-1, 1)\n",
    "    scaled_accuracies_array = scaler.fit_transform(accuracies_array)\n",
    "    scaled_accuracies = {k: v for k, v in zip(currencies, scaled_accuracies_array.flatten())}\n",
    "    total_scaled_accuracies = sum(scaled_accuracies.values())\n",
    "    normalized_accuracies = {k: v / total_scaled_accuracies for k, v in scaled_accuracies.items()}\n",
    "\n",
    "    return normalized_accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_returns(price, returns, label):\n",
    "    # Create a boolean mask indicating whether the next day's return is positive\n",
    "    next_day_positive = returns > 0\n",
    "    next_day_neutral = abs(returns) < confidence_threshold\n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Plot the price data\n",
    "    ax.plot(list(range(len(price))), price)\n",
    "\n",
    "    # Loop through the boolean mask and plot green or red vertical areas as appropriate\n",
    "    for i, positive in enumerate(next_day_positive):\n",
    "        if positive and not next_day_neutral[i]:\n",
    "            ax.axvspan(i, i+1, color='g', alpha=0.2)\n",
    "        elif (not positive and not next_day_neutral[i]):\n",
    "            ax.axvspan(i, i+1, color='r', alpha=0.2)\n",
    "        else:\n",
    "            ax.axvspan(i, i+1, color='grey', alpha=0.2)\n",
    "\n",
    "    # Set the plot title and axis labels\n",
    "    ax.set_title(\"\")\n",
    "    ax.set_xlabel(\"Weeks\")\n",
    "    ax.set_ylabel(f\"Return of {label}\")\n",
    "    fig.savefig(f\"signals_pca/{label}.png\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curncy in list(data.keys()):\n",
    "    plot_returns((1+pd.Series(results[curncy][\"y_test\"][:-1])).cumprod(), results[curncy][\"y_test\"], curncy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signals(returns):\n",
    "    # Create a boolean mask indicating whether the next day's return is positive\n",
    "    next_day_positive = returns > 0\n",
    "    next_day_neutral = abs(returns) < confidence_threshold\n",
    "    signals = []\n",
    "    # Loop through the boolean mask and plot green or red vertical areas as appropriate\n",
    "    for i, positive in enumerate(next_day_positive):\n",
    "        if positive and not next_day_neutral[i]:\n",
    "            signals.append(1)\n",
    "        elif (not positive and not next_day_neutral[i]):\n",
    "            signals.append(-1)\n",
    "        else:\n",
    "            signals.append(0)\n",
    "            \n",
    "\n",
    "    return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(lagged_returns, y_test):\n",
    "    non_cum_returns = []\n",
    "    returns = [1]\n",
    "    signals = get_signals(lagged_returns)\n",
    "    cum_returns = 1\n",
    "    correct = 0\n",
    "    for i in range(len(lagged_returns)):\n",
    "        if signals[i] == 1:\n",
    "            cum_returns *= (1+y_test[i]) #1 * 1.04 \n",
    "            non_cum_returns.append(y_test[i])\n",
    "            if y_test[i] > 0:\n",
    "                correct += 1\n",
    "        elif signals[i] == -1:\n",
    "            cum_returns *= (1+(-1*y_test[i])) #1 * 1+(-1*.04 = 1 * 0.96\n",
    "            non_cum_returns.append(-1*y_test[i])\n",
    "            if y_test[i] < 0:\n",
    "                correct += 1\n",
    "        else:\n",
    "            # print('here')\n",
    "            non_cum_returns.append(0)\n",
    "            if abs(y_test[i]) < confidence_threshold:\n",
    "                correct += 1\n",
    "            \n",
    "        returns.append(cum_returns)\n",
    "\n",
    "    return pd.Series(returns), pd.Series(non_cum_returns), signals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_returns = {}\n",
    "\n",
    "for curncy in list(data.keys()):\n",
    "    returns, non_cum_returns , signals = calculate_returns(results[curncy][\"training_predictions\"], results[curncy][\"y_train\"])\n",
    "    training_returns[curncy] = non_cum_returns\n",
    "\n",
    "max_length = max([len(returns) for returns in training_returns.values()])\n",
    "\n",
    "\n",
    "all_training_returns_df = pd.DataFrame()\n",
    "for currency_pair, returns in training_returns.items():\n",
    "    padded_returns = [[float('nan')] * (max_length - len(returns))] + [returns]\n",
    "    padded_returns[0].extend(returns)\n",
    "    temp_df = pd.DataFrame({currency_pair: padded_returns[0]})\n",
    "    all_training_returns_df = pd.concat([all_training_returns_df, temp_df], axis=1)\n",
    "\n",
    "\n",
    "all_training_returns_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_returns_no_nan = all_training_returns_df.dropna(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_return_means = training_returns_no_nan.mean()\n",
    "training_return_cov = training_returns_no_nan.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def mean_variance_optimization(mean_returns, cov_matrix, risk_aversion_lambda, short_allowed=True):\n",
    "    \"\"\"\n",
    "    This function performs mean-variance optimization using the input mean returns, \n",
    "    covariance matrix as a Pandas DataFrame, and a lambda risk aversion parameter.\n",
    "\n",
    "    Parameters:\n",
    "    - mean_returns: A list or numpy array of mean returns for assets\n",
    "    - cov_matrix: A Pandas DataFrame representing the covariance matrix for assets\n",
    "    - risk_aversion_lambda: A float representing the risk aversion parameter\n",
    "    - short_allowed: A boolean to indicate if short selling is allowed (default: False)\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the optimal weights, expected return, and risk\n",
    "    \"\"\"\n",
    "\n",
    "    num_assets = len(mean_returns)\n",
    "    if short_allowed:\n",
    "        bounds = [(-.2, 0.2) for _ in range(num_assets)]\n",
    "    else:\n",
    "        bounds = [(0, 1) for _ in range(num_assets)]\n",
    "    \n",
    "    constraints = (\n",
    "        {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1},\n",
    "    )\n",
    "\n",
    "    def objective_function(weights):\n",
    "        return -1 * (np.dot(weights, mean_returns) - 0.5 * risk_aversion_lambda * np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "\n",
    "    initial_guess = np.repeat(1 / num_assets, num_assets)\n",
    "    result = minimize(\n",
    "        objective_function,\n",
    "        initial_guess,\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "    )\n",
    "\n",
    "    optimal_weights = result.x\n",
    "    expected_return = np.dot(optimal_weights, mean_returns)\n",
    "    risk = np.sqrt(np.dot(optimal_weights.T, np.dot(cov_matrix, optimal_weights)))\n",
    "\n",
    "    return {\n",
    "        \"optimal_weights\": optimal_weights,\n",
    "        \"expected_return\": expected_return,\n",
    "        \"risk\": risk,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results = mean_variance_optimization(training_return_means, training_return_cov, 100, True)\n",
    "weights = opt_results[\"optimal_weights\"]\n",
    "weights_dict = {}\n",
    "sum_w = 0\n",
    "for i, currency_pair in enumerate(training_return_means.index):\n",
    "    weights_dict[currency_pair] = weights[i].round(2)\n",
    "    sum_w += weights[i].round(2)\n",
    "weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_returns_dict = {}\n",
    " \n",
    "for curncy in list(data.keys()):\n",
    "    returns, non_cum_returns , signals = calculate_returns(results[curncy][\"lagged_returns\"], results[curncy][\"y_test\"])\n",
    "    all_returns_dict[curncy] = non_cum_returns\n",
    "\n",
    "max_length = max([len(returns) for returns in all_returns_dict.values()])\n",
    "\n",
    "\n",
    "all_returns_df = pd.DataFrame()\n",
    "for currency_pair, returns in all_returns_dict.items():\n",
    "    padded_returns = [[float('nan')] * (max_length - len(returns))] + [returns]\n",
    "    padded_returns[0].extend(returns)\n",
    "    temp_df = pd.DataFrame({currency_pair: padded_returns[0]})\n",
    "    all_returns_df = pd.concat([all_returns_df, temp_df], axis=1)\n",
    "\n",
    "\n",
    "all_returns_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currency portfolio\n",
    "currncy_returns_dict = {}\n",
    " \n",
    "for curncy in list(data.keys()):\n",
    "    returns, non_cum_returns , signals = calculate_returns(results[curncy][\"lagged_returns\"], results[curncy][\"y_test\"])\n",
    "    currncy_returns_dict[curncy] = results[curncy][\"y_test\"]\n",
    "\n",
    "max_length = max([len(returns) for returns in currncy_returns_dict.values()])\n",
    "\n",
    "\n",
    "currncy_returns_df = pd.DataFrame()\n",
    "for currency_pair, returns in currncy_returns_dict.items():\n",
    "    padded_returns = [[float('nan')] * (max_length - len(returns))] + [returns]\n",
    "    padded_returns[0].extend(returns)\n",
    "    temp_df = pd.DataFrame({currency_pair: padded_returns[0]})\n",
    "    currncy_returns_df = pd.concat([currncy_returns_df, temp_df], axis=1)\n",
    "\n",
    "\n",
    "currncy_returns_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the all_returns_df and for each row find which columns are not nan\n",
    "curncy_equal_weight_port = []\n",
    "for index, row in currncy_returns_df.iterrows():\n",
    "    good_currencies = []\n",
    "    for curncy in list(currncy_returns_df.keys()):\n",
    "        if not pd.isna(row.loc[curncy]):\n",
    "            good_currencies.append(curncy)\n",
    "    weights = {curncy: 1/len(good_currencies) for curncy in good_currencies}\n",
    "    daily_ret = 0\n",
    "    for curncy in good_currencies:\n",
    "        daily_ret += row.loc[curncy] * weights[curncy]\n",
    "    curncy_equal_weight_port.append(daily_ret)\n",
    "curncy_equal_weight_port = pd.Series(curncy_equal_weight_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_returns = (1+ (pd.Series(curncy_equal_weight_port))).cumprod()\n",
    "dates = data[list(data.keys())[0]].index[-len(cum_returns):]\n",
    "plt.plot(dates, cum_returns)\n",
    "plt.title(f\"Equal Weighted Currency Returns\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cummulative Returns\")\n",
    "sortino_ratio(daily_portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equal weighted\n",
    "daily_portfolio_returns_equal_weight = []\n",
    "for index, row in all_returns_df.iterrows():\n",
    "    good_currencies = []\n",
    "    for curncy in list(all_returns_df.keys()):\n",
    "        if not pd.isna(row.loc[curncy]):\n",
    "            good_currencies.append(curncy)\n",
    "    weights = {curncy: 1/len(good_currencies) for curncy in good_currencies}\n",
    "    daily_ret = 0\n",
    "    for curncy in good_currencies:\n",
    "        daily_ret += row.loc[curncy] * weights[curncy]\n",
    "    daily_portfolio_returns_equal_weight.append(daily_ret)\n",
    "daily_portfolio_returns_equal_weight = pd.Series(daily_portfolio_returns_equal_weight)\n",
    "(1+pd.Series(daily_portfolio_returns_equal_weight)).cumprod().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the all_returns_df and for each row find which columns are not nan\n",
    "model_error_weight_with_short_portfolio_returns = []\n",
    "for index, row in all_returns_df.iterrows():\n",
    "    good_currencies = []\n",
    "    for curncy in list(all_returns_df.keys()):\n",
    "        if not pd.isna(row.loc[curncy]):\n",
    "            good_currencies.append(curncy)\n",
    "    weights = get_scaled_weights_wiht_shorts(good_currencies)\n",
    "    # weights = weights_dict\n",
    "    daily_ret = 0\n",
    "    for curncy in good_currencies:\n",
    "        daily_ret += row.loc[curncy] * weights[curncy]\n",
    "    model_error_weight_with_short_portfolio_returns.append(daily_ret)\n",
    "model_error_weight_with_short_portfolio_returns = pd.Series(model_error_weight_with_short_portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_model_error_weight_with_short_portfolio_returns= (1+ (pd.Series(model_error_weight_with_short_portfolio_returns))).cumprod()\n",
    "dates = data[list(data.keys())[0]].index[-len(cum_model_error_weight_with_short_portfolio_returns):]\n",
    "plt.plot(dates, cum_model_error_weight_with_short_portfolio_returns)\n",
    "plt.title(f\"Portfolio Returns (Model Error Weighted with Shorts))\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cummulative Returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the all_returns_df and for each row find which columns are not nan\n",
    "mean_var_portfolio_returns = []\n",
    "for index, row in all_returns_df.iterrows():\n",
    "    good_currencies = []\n",
    "    for curncy in list(all_returns_df.keys()):\n",
    "        if not pd.isna(row.loc[curncy]):\n",
    "            good_currencies.append(curncy)\n",
    "    # weights = get_scaled_weights(good_currencies)\n",
    "    weights = weights_dict\n",
    "    daily_ret = 0\n",
    "    for curncy in good_currencies:\n",
    "        daily_ret += row.loc[curncy] * weights[curncy]\n",
    "    mean_var_portfolio_returns.append(daily_ret)\n",
    "mean_var_portfolio_returns = pd.Series(mean_var_portfolio_returns) + 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_returns_mean_var_port = (1+ (pd.Series(mean_var_portfolio_returns))).cumprod()\n",
    "dates = data[list(data.keys())[0]].index[-len(cum_returns_mean_var_port):]\n",
    "plt.plot(dates, cum_returns_mean_var_port)\n",
    "plt.title(f\"Mean Variance Portfolio Returns With Shorting (Lambda = 100)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cummulative Returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the all_returns_df and for each row find which columns are not nan\n",
    "daily_portfolio_returns = []\n",
    "for index, row in all_returns_df.iterrows():\n",
    "    good_currencies = []\n",
    "    for curncy in list(all_returns_df.keys()):\n",
    "        if not pd.isna(row.loc[curncy]):\n",
    "            good_currencies.append(curncy)\n",
    "    weights = get_scaled_weights(good_currencies)\n",
    "    daily_ret = 0\n",
    "    for curncy in good_currencies:\n",
    "        daily_ret += row.loc[curncy] * weights[curncy]\n",
    "    daily_portfolio_returns.append(daily_ret)\n",
    "daily_portfolio_returns = pd.Series(daily_portfolio_returns) + 0.0025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortino_ratio(series, N=0,rf=0.04):\n",
    "    N = len(series)\n",
    "    mean = series.mean() * N -rf\n",
    "    std_neg = series[series<0].std()*np.sqrt(N)\n",
    "    return mean/std_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_returns_port = (1+ (pd.Series(daily_portfolio_returns))).cumprod()\n",
    "dates = data[list(data.keys())[0]].index[-len(cum_returns_port):]\n",
    "plt.plot(dates, cum_returns_port)\n",
    "plt.title(f\"Portfolio Returns\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cummulative Returns\")\n",
    "# sortino_ratio(daily_portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(dates, cum_returns_port-cum_returns)\n",
    "plt.title(f\"Model Portfolio - Equal Weighted Portfolio (Excess Returns)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cummulative Returns\")\n",
    "# sortino_ratio(daily_portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def annualized_return(returns):\n",
    "    returns = np.array(returns)\n",
    "    annualized_ret = (1 + returns).prod()**(12 / len(returns)) - 1\n",
    "    return annualized_ret\n",
    "\n",
    "def annualized_volatility(returns):\n",
    "    returns = np.array(returns)\n",
    "    annualized_vol = np.std(returns) * np.sqrt(12)\n",
    "    return annualized_vol\n",
    "\n",
    "def sortino_ratio(series, N=0,rf=0.00):\n",
    "    N = len(series)\n",
    "    mean = series.mean() * N -rf\n",
    "    std_neg = series[series<0].std()*np.sqrt(N)\n",
    "    return mean/std_neg\n",
    "\n",
    "def sharpe_ratio(returns, risk_free_rate=0.0):\n",
    "    annual_ret = annualized_return(returns)\n",
    "    annual_vol = annualized_volatility(returns)\n",
    "    sharpe = (annual_ret - risk_free_rate) / annual_vol\n",
    "    return sharpe\n",
    "\n",
    "def max_drawdown(returns):\n",
    "    returns = np.array(returns)\n",
    "    cum_returns = (1 + returns).cumprod()\n",
    "    peak = np.maximum.accumulate(cum_returns)\n",
    "    drawdowns = (cum_returns - peak) / peak\n",
    "    max_drawdown = np.min(drawdowns)\n",
    "    return max_drawdown\n",
    "\n",
    "def value_at_risk(returns, alpha=0.05):\n",
    "    returns = np.array(returns)\n",
    "    var = np.percentile(returns, 100 * alpha)\n",
    "    return var\n",
    "\n",
    "def cvar(returns, alpha=0.05):\n",
    "    returns = np.array(returns)\n",
    "    var = value_at_risk(returns, alpha)\n",
    "    cvar = returns[returns <= var].mean()\n",
    "    return cvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{ 'Annualized Return': annualized_return(daily_portfolio_returns),\n",
    " 'Annualized Volatility': annualized_volatility(daily_portfolio_returns),\n",
    " 'Sharpe Ratio': sharpe_ratio(daily_portfolio_returns),\n",
    " 'Sortino Ratio': sortino_ratio(daily_portfolio_returns),\n",
    " 'Max Drawdown': max_drawdown(daily_portfolio_returns),\n",
    " 'Value at Risk': value_at_risk(daily_portfolio_returns),\n",
    " 'Conditional Value at Risk': cvar(daily_portfolio_returns)\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(returns):\n",
    "    return {\n",
    "        'Annualized Return': annualized_return(returns),\n",
    "        'Annualized Volatility': annualized_volatility(returns),\n",
    "        'Sharpe Ratio': sharpe_ratio(returns),\n",
    "        'Sortino Ratio': sortino_ratio(returns),\n",
    "        'Max Drawdown': max_drawdown(returns),\n",
    "        'Value at Risk': value_at_risk(returns),\n",
    "        'Conditional Value at Risk': cvar(returns)\n",
    "    }\n",
    "# { 'Annualized Return': annualized_return(daily_portfolio_returns),\n",
    "#  'Annualized Volatility': annualized_volatility(daily_portfolio_returns),\n",
    "#  'Sharpe Ratio': sharpe_ratio(daily_portfolio_returns),\n",
    "#  'Sortino Ratio': sortino_ratio(daily_portfolio_returns),\n",
    "#  'Max Drawdown': max_drawdown(daily_portfolio_returns),\n",
    "#  'Value at Risk': value_at_risk(daily_portfolio_returns),\n",
    "#  'Conditional Value at Risk': cvar(daily_portfolio_returns)\n",
    "#  }\n",
    "get_stats(mean_var_portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stats(model_error_weight_with_short_portfolio_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Annualized Return\": annualized_return(curncy_equal_weight_port),\n",
    "    \"Annualized Volatility\": annualized_volatility(curncy_equal_weight_port),\n",
    "    \"Sharpe Ratio\": sharpe_ratio(curncy_equal_weight_port),\n",
    "    \"Sortino Ratio\": sortino_ratio(curncy_equal_weight_port),\n",
    "    \"Max Drawdown\": max_drawdown(curncy_equal_weight_port),\n",
    "    \"Value at Risk\": value_at_risk(curncy_equal_weight_port),\n",
    "    \"Conditional Value at Risk\": cvar(curncy_equal_weight_port),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Annualized Return\": annualized_return(daily_portfolio_returns) - annualized_return(curncy_equal_weight_port),\n",
    "    \"Annualized Volatility\": annualized_volatility(daily_portfolio_returns) - annualized_volatility(curncy_equal_weight_port),\n",
    "    \"Sharpe Ratio\": sharpe_ratio(daily_portfolio_returns) - sharpe_ratio(curncy_equal_weight_port),\n",
    "    \"Sortino Ratio\": sortino_ratio(daily_portfolio_returns) - sortino_ratio(curncy_equal_weight_port),\n",
    "    \"Max Drawdown\": max_drawdown(daily_portfolio_returns) - max_drawdown(curncy_equal_weight_port),\n",
    "    \"Value at Risk\": value_at_risk(daily_portfolio_returns) - value_at_risk(curncy_equal_weight_port),\n",
    "    \"Conditional Value at Risk\": cvar(daily_portfolio_returns) - cvar(curncy_equal_weight_port),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transactions(signals):\n",
    "    transactions = [signals[0]]  # Initialize the transactions list with 0 as the first value\n",
    "\n",
    "    # Loop through the signals list starting from the second element\n",
    "    for prev_signal, current_signal in zip(signals[:-1], signals[1:]):\n",
    "        if prev_signal != current_signal:\n",
    "            if current_signal == 0:\n",
    "                if prev_signal == 1:\n",
    "                    transactions.append(-1)  # Sell transaction\n",
    "                elif prev_signal == -1:\n",
    "                    transactions.append(1)  # Buy transaction\n",
    "            elif prev_signal == 0 or prev_signal == -1 and current_signal == 1:\n",
    "                transactions.append(1)  # Buy transaction\n",
    "            elif prev_signal == 0 or prev_signal == 1 and current_signal == -1:\n",
    "                transactions.append(-1)  # Short transaction\n",
    "        else:\n",
    "            transactions.append(0)  # No transaction\n",
    "    return transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curncy in [list(data.keys())][0]:\n",
    "    returns, non_cum_returns, signals = calculate_returns(results[curncy][\"lagged_returns\"], results[curncy][\"y_test\"])\n",
    "    transactions = get_transactions(signals)\n",
    "    indicies = results[curncy][\"y_test\"].index[-(len(returns)+1):]\n",
    "    plt.plot(indicies, (1+pd.Series([0, *results[curncy][\"y_test\"]])).cumprod()[:-1], label=f\"{curncy} Price Returns\")\n",
    "    # range(len(returns))\n",
    "    plt.plot(indicies, returns[:-1], label=f\"{curncy} Strategy Returns\")\n",
    "    #axvline for each transaction in transactions. If it is 0 no axvline. If it is 1 then green axvline. If it is -1 then red axvline\n",
    "    for i, transaction in enumerate(transactions):\n",
    "        if transaction == 1:\n",
    "            plt.axvline(x=indicies[i], color='g', linestyle='--', alpha=0.3)\n",
    "        elif transaction == -1:\n",
    "            plt.axvline(x=indicies[i], color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.title(f\"{curncy} Strategy Returns\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"returns_pca/{curncy}.png\")\n",
    "    plt.show()\n",
    "    # plt.cla()\n",
    "    # plt.clf()\n",
    "    \n",
    "    np.mean(returns) / np.std(returns) * np.sqrt(len(returns))\n",
    "    print(non_cum_returns.mean()/non_cum_returns.std())\n",
    "    print(np.mean(non_cum_returns) / np.std(non_cum_returns) * np.sqrt(len(non_cum_returns)))\n",
    "    print(np.mean(results[curncy][\"y_test\"]) / np.std(results[curncy][\"y_test\"]) * np.sqrt(len(results[curncy][\"y_test\"])))\n",
    "    print(returns.iloc[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
